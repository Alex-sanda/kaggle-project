{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 3 of kaggle-sentiment analysis\n",
    "\n",
    "\n",
    "1.restore the word2vec model and get word vector\n",
    "\n",
    "2.combine all word vector in one review into one vector\n",
    "\n",
    "3.use random forest to generate a sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#restore the trained model from word2vec part\n",
    "model = Word2Vec.load(\"300features_40minwords_10contex\")\n",
    "\n",
    "#shape of the word vector\n",
    "#more about word2vec : \n",
    "#     https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"flower\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From Words To Paragraphs, Attempt 1: Vector Averaging\n",
    "\n",
    "One challenge with the IMDB dataset is the variable-length reviews. We need to find a way to take individual word vectors and transform them into a feature set that is the same length for every review.\n",
    "\n",
    "Since each word is a vector in 300-dimensional space, we can use vector operations to combine the words in each review. One method we tried was to simply average the word vectors in a given review (for this purpose, we removed stop words, which would just add noise).\n",
    "\n",
    "The following code averages the feature vectors, building on our code from Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words,model,num_features):\n",
    "    \n",
    "    #build up a zero feature vector\n",
    "    featureVecs = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #indicating how many words to average\n",
    "    nwords = 0\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            featureVecs = np.add(featureVecs,model.wv[word])\n",
    "            nwords += 1\n",
    "    featureVecs = np.divide(featureVecs,nwords)\n",
    "    return featureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVec(reviews,model,num_features):\n",
    "    \n",
    "    #initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    #initialize a zero matrix \n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        if (counter+1)%1000 ==0:\n",
    "            print(\"Review %d of %d\" % (counter,len(reviews)))\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review,model,num_features)\n",
    "        counter += 1\n",
    "    return  reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean review with option : whether remove stop word\n",
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    #Function to convert a document to a sequence of words\n",
    "    #1.remove Html\n",
    "    raw_review = BeautifulSoup(review).get_text()\n",
    "    #2.remove NON-charaters\n",
    "    cleaned_words = re.sub(\"[^a-zA-Z]\",\" \",raw_review)\n",
    "    #3.lower case and split review into word list\n",
    "    words = cleaned_words.lower().split()\n",
    "    #4.remove stopwords if said\n",
    "    if remove_stopwords == True:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/alex/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 999 of 25000\n",
      "Review 1999 of 25000\n",
      "Review 2999 of 25000\n",
      "Review 3999 of 25000\n",
      "Review 4999 of 25000\n",
      "Review 5999 of 25000\n",
      "Review 6999 of 25000\n",
      "Review 7999 of 25000\n",
      "Review 8999 of 25000\n",
      "Review 9999 of 25000\n",
      "Review 10999 of 25000\n",
      "Review 11999 of 25000\n",
      "Review 12999 of 25000\n",
      "Review 13999 of 25000\n",
      "Review 14999 of 25000\n",
      "Review 15999 of 25000\n",
      "Review 16999 of 25000\n",
      "Review 17999 of 25000\n",
      "Review 18999 of 25000\n",
      "Review 19999 of 25000\n",
      "Review 20999 of 25000\n",
      "Review 21999 of 25000\n",
      "Review 22999 of 25000\n",
      "Review 23999 of 25000\n",
      "Review 24999 of 25000\n",
      "Review 999 of 25000\n",
      "Review 1999 of 25000\n",
      "Review 2999 of 25000\n",
      "Review 3999 of 25000\n",
      "Review 4999 of 25000\n",
      "Review 5999 of 25000\n",
      "Review 6999 of 25000\n",
      "Review 7999 of 25000\n",
      "Review 8999 of 25000\n",
      "Review 9999 of 25000\n",
      "Review 10999 of 25000\n",
      "Review 11999 of 25000\n",
      "Review 12999 of 25000\n",
      "Review 13999 of 25000\n",
      "Review 14999 of 25000\n",
      "Review 15999 of 25000\n",
      "Review 16999 of 25000\n",
      "Review 17999 of 25000\n",
      "Review 18999 of 25000\n",
      "Review 19999 of 25000\n",
      "Review 20999 of 25000\n",
      "Review 21999 of 25000\n",
      "Review 22999 of 25000\n",
      "Review 23999 of 25000\n",
      "Review 24999 of 25000\n"
     ]
    }
   ],
   "source": [
    "# train review feature vectors\n",
    "cleaned_train_reviews = []\n",
    "train = pd.read_csv(\"./dataset/labeledTrainData.tsv\",\n",
    "                    header=0,\n",
    "                    delimiter='\\t',\n",
    "                    quoting=3)\n",
    "for i in range(train[\"review\"].size):\n",
    "    cleaned_train_reviews.append(review_to_wordlist(train[\"review\"][i],remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVec(cleaned_train_reviews,model,300)\n",
    "    \n",
    "#test review feature vectors\n",
    "cleaned_test_reviews = []\n",
    "test = pd.read_csv(\"./dataset/testData.tsv\",\n",
    "                   header=0,delimiter='\\t',\n",
    "                   quoting=3)\n",
    "for i in range(test[\"review\"].size):\n",
    "    cleaned_test_reviews.append(review_to_wordlist(test[\"review\"][i],remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVec(cleaned_test_reviews,model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize random forest classifier\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#train the classifier\n",
    "forest.fit(trainDataVecs,train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make prediction and store results\n",
    "result = forest.predict(testDataVecs)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"],\"sentiment\":result})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\",index=False,quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From Words to Paragraphs, Attempt 2: Clustering \n",
    "\n",
    "Word2Vec creates clusters of semantically related words, so another possible approach is to exploit the similarity of words within a cluster. Grouping vectors in this way is known as \"vector quantization.\" To accomplish this, we first need to find the centers of the word clusters, which we can do by using a clustering algorithm such as K-Means.\n",
    "\n",
    "In K-Means, the one parameter we need to set is \"K,\" or the number of clusters. How should we decide how many clusters to create? Trial and error suggested that small clusters, with an average of only 5 words or so per cluster, gave better results than large clusters with many words. Clustering code is given below. We use scikit-learn to perform our K-Means.\n",
    "\n",
    "K-Means clustering with large K can be very slow; the following code took more than 40 minutes on my computer. Below, we set a timer around the K-Means function to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:357.40642619132996 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#ndarray word vector\n",
    "word_vectors = model.wv.syn0\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "num_clusters = word_vectors.shape[0]//5\n",
    "\n",
    "#initialize an KMeans object\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "#make clusters for word vectors\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "end = time.time()\n",
    "print(\"Time taken for K Means clustering:{} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster :0\n",
      "['blue', 'green', 'glowing', 'soylent']\n",
      "cluster :1\n",
      "['christopher', 'walken', 'baldwin', 'reeve', 'lambert', 'plummer']\n",
      "cluster :2\n",
      "['pursuit', 'midst', 'meantime']\n",
      "cluster :3\n",
      "['awkward', 'lazy', 'clumsy', 'phony', 'deliberate', 'limp', 'misplaced', 'strained', 'calculated', 'shabby', 'haphazard', 'functional', 'repetitious']\n",
      "cluster :4\n",
      "['iraqi']\n",
      "cluster :5\n",
      "['infamous', 'notorious', 'acclaimed', 'iconic', 'celebrated', 'influential', 'quintessential', 'prolific', 'pioneer', 'revered']\n",
      "cluster :6\n",
      "['dagger']\n",
      "cluster :7\n",
      "['kate', 'melissa', 'ashley', 'olsen', 'farrah', 'beckinsale', 'winslet', 'latifah']\n",
      "cluster :8\n",
      "['concludes', 'engages', 'replaces', 'guides', 'undergoes', 'indulges']\n",
      "cluster :9\n",
      "['front', 'block', 'cliff', 'roof', 'desk', 'deck', 'staircase', 'branch', 'balcony', 'lawn', 'heel', 'tips', 'dock', 'dangling']\n"
     ]
    }
   ],
   "source": [
    "#show clustering result\n",
    "word_centroid_map = dict(zip(model.wv.index2word,idx))\n",
    "for cluster in range(0,10):\n",
    "    words = []\n",
    "    for i in range(len(word_centroid_map)):\n",
    "        if list(word_centroid_map.values())[i] == cluster:\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(\"cluster :{}\".format(cluster))\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist,word_centroid_map):\n",
    "    #Very much alike the bag of words idea\n",
    "    #count the frequency of clusters happened in a review\n",
    "    num_centroid = max(word_centroid_map.values())+1\n",
    "    #create a zero vector(bag of cluster)\n",
    "    bag_of_centroid = np.zeros(num_centroid,dtype=\"float32\")\n",
    "    #loop over the words in the review if the word is in the vocabulary,\n",
    "    #find the cluster and plus one \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroid[index] += 1\n",
    "    return bag_of_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/alex/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# train review feature vectors\n",
    "cleaned_train_reviews = []\n",
    "train = pd.read_csv(\"./dataset/labeledTrainData.tsv\",\n",
    "                    header=0,\n",
    "                    delimiter='\\t',\n",
    "                    quoting=3)\n",
    "for i in range(train[\"review\"].size):\n",
    "    cleaned_train_reviews.append(review_to_wordlist(train[\"review\"][i],remove_stopwords=True))\n",
    "    \n",
    "#test review feature vectors\n",
    "cleaned_test_reviews = []\n",
    "test = pd.read_csv(\"./dataset/testData.tsv\",\n",
    "                   header=0,delimiter='\\t',\n",
    "                   quoting=3)\n",
    "for i in range(test[\"review\"].size):\n",
    "    cleaned_test_reviews.append(review_to_wordlist(test[\"review\"][i],remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize a zero matrix to store \"bag of cluster\"features for reviews\n",
    "train_centroids = np.zeros((train[\"review\"].size,num_clusters),dtype=\"float32\")\n",
    "\n",
    "#\n",
    "counter = 0\n",
    "for review in cleaned_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review,\n",
    "                                                       word_centroid_map)\n",
    "    counter += 1\n",
    "#initialize a zero matrix to store \"bag of cluster\"features for reviews\n",
    "test_centroids = np.zeros((test[\"review\"].size,num_clusters),dtype=\"float32\")\n",
    "\n",
    "counter = 0\n",
    "for review in cleaned_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids(review,\n",
    "                                                     word_centroid_map)\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a random forest and extract prediction\n",
    "\n",
    "#initialize the random forest model\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#train the model with bag of cluster feature\n",
    "forest.fit(train_centroids,train[\"sentiment\"])\n",
    "\n",
    "#make prediction\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "#write the test results\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"],\"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
